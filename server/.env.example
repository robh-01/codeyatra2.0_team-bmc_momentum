# Server Port
PORT=3001

# Ollama Configuration
# Make sure Ollama is running: ollama serve
OLLAMA_URL=http://127.0.0.1:11434

# Model to use (run: ollama pull <model> first)
# Options: qwen3:4b, llama3.2, gemma3:4b, mistral, etc.
OLLAMA_MODEL=qwen3:4b
